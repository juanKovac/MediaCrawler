# -*- coding: utf-8 -*-
# å£°æ˜ï¼šæœ¬ä»£ç ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚ä½¿ç”¨è€…åº”éµå®ˆä»¥ä¸‹åŸåˆ™ï¼š
# 1. ä¸å¾—ç”¨äºä»»ä½•å•†ä¸šç”¨é€”ã€‚
# 2. ä½¿ç”¨æ—¶åº”éµå®ˆç›®æ ‡å¹³å°çš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtè§„åˆ™ã€‚
# 3. ä¸å¾—è¿›è¡Œå¤§è§„æ¨¡çˆ¬å–æˆ–å¯¹å¹³å°é€ æˆè¿è¥å¹²æ‰°ã€‚
# 4. åº”åˆç†æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…ç»™ç›®æ ‡å¹³å°å¸¦æ¥ä¸å¿…è¦çš„è´Ÿæ‹…ã€‚
# 5. ä¸å¾—ç”¨äºä»»ä½•éæ³•æˆ–ä¸å½“çš„ç”¨é€”ã€‚
#
# è¯¦ç»†è®¸å¯æ¡æ¬¾è¯·å‚é˜…é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„LICENSEæ–‡ä»¶ã€‚
# ä½¿ç”¨æœ¬ä»£ç å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä¸Šè¿°åŸåˆ™å’ŒLICENSEä¸­çš„æ‰€æœ‰æ¡æ¬¾ã€‚

from flask import Flask, render_template, request, jsonify, send_file
import asyncio
import threading
import uuid
import os
import json
import time
from datetime import datetime
import sys
from typing import Dict, Any, Optional
import pandas as pd
import glob

# å¯¼å…¥åŸæœ‰çš„çˆ¬è™«æ¨¡å—
import config
import db
from main import CrawlerFactory
from base.base_crawler import AbstractCrawler

app = Flask(__name__, template_folder='web_templates', static_folder='web_static')

# å­˜å‚¨ä»»åŠ¡çŠ¶æ€çš„å­—å…¸
tasks = {}

# å¹³å°é…ç½®
PLATFORM_OPTIONS = {
    'xhs': 'å°çº¢ä¹¦',
    'dy': 'æŠ–éŸ³',
    'ks': 'å¿«æ‰‹',
    'bili': 'å“”å“©å“”å“©',
    'wb': 'å¾®åš',
    'tieba': 'ç™¾åº¦è´´å§',
    'zhihu': 'çŸ¥ä¹'
}

# ç™»å½•æ–¹å¼é…ç½®
LOGIN_TYPE_OPTIONS = {
    'qrcode': 'äºŒç»´ç ç™»å½•',
    'phone': 'æ‰‹æœºå·ç™»å½•',
    'cookie': 'Cookieç™»å½•'
}

# çˆ¬å–ç±»å‹é…ç½®
CRAWLER_TYPE_OPTIONS = {
    'search': 'å…³é”®è¯æœç´¢',
    'detail': 'å¸–å­è¯¦æƒ…',
    'creator': 'åˆ›ä½œè€…ä¸»é¡µ'
}

# æ•°æ®ä¿å­˜æ–¹å¼é…ç½®
SAVE_DATA_OPTIONS = {
    'json': 'JSONæ–‡ä»¶',
    'csv': 'CSVæ–‡ä»¶',
    'sqlite': 'SQLiteæ•°æ®åº“',
    'db': 'MySQLæ•°æ®åº“'
}

class TaskStatus:
    """ä»»åŠ¡çŠ¶æ€ç±»"""
    PENDING = 'pending'      # ç­‰å¾…ä¸­
    RUNNING = 'running'      # è¿è¡Œä¸­
    COMPLETED = 'completed'  # å·²å®Œæˆ
    FAILED = 'failed'        # å¤±è´¥
    STOPPED = 'stopped'      # å·²åœæ­¢

def run_crawler_task(task_id: str, crawler_config: Dict[str, Any]):
    """åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œçˆ¬è™«ä»»åŠ¡"""
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
        tasks[task_id]['status'] = TaskStatus.RUNNING
        tasks[task_id]['start_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        tasks[task_id]['started_at'] = time.time()
        
        # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯æ¥è¿è¡Œå¼‚æ­¥çˆ¬è™«ä»»åŠ¡
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(execute_crawler_task(task_id, crawler_config))
        finally:
            loop.close()
        
    except Exception as e:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        tasks[task_id]['status'] = TaskStatus.FAILED
        tasks[task_id]['error'] = str(e)
        tasks[task_id]['message'] = f'ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {str(e)}'
        tasks[task_id]['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        tasks[task_id]['completed_at'] = time.time()
        print(f"Task {task_id} failed with error: {e}")


async def execute_crawler_task(task_id: str, crawler_config: Dict[str, Any]):
    """æ‰§è¡Œçˆ¬è™«ä»»åŠ¡çš„å¼‚æ­¥å‡½æ•°"""
    crawler: Optional[AbstractCrawler] = None
    
    try:
        # è®¾ç½®é…ç½®å‚æ•°
        config.PLATFORM = crawler_config['platform']
        config.LOGIN_TYPE = crawler_config['login_type']
        config.CRAWLER_TYPE = crawler_config['crawler_type']
        config.KEYWORDS = crawler_config['keywords']
        config.START_PAGE = crawler_config['start_page']
        config.ENABLE_GET_COMMENTS = crawler_config['get_comments']
        config.ENABLE_GET_SUB_COMMENTS = crawler_config['get_sub_comments']
        config.SAVE_DATA_OPTION = crawler_config['save_data_option']
        config.CRAWLER_MAX_NOTES_COUNT = crawler_config['max_notes_count']
        config.COOKIES = crawler_config.get('cookies', '')
        
        # å¤„ç†åˆ›ä½œè€…æ¨¡å¼çš„ç”¨æˆ·ID
        if crawler_config['crawler_type'] == 'creator' and crawler_config.get('creator_id'):
            # å°†ç”¨æˆ·è¾“å…¥çš„åˆ›ä½œè€…IDè®¾ç½®åˆ°é…ç½®ä¸­
            if crawler_config['platform'] == 'xhs':
                config.XHS_CREATOR_ID_LIST = [crawler_config['creator_id']]
        
        # åˆå§‹åŒ–æ•°æ®åº“ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if config.SAVE_DATA_OPTION in ["db", "sqlite"]:
            await db.init_db()
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = CrawlerFactory.create_crawler(platform=config.PLATFORM)
        
        # å¯åŠ¨çˆ¬è™«
        await crawler.start()
        
        # è·å–è¾“å‡ºæ–‡ä»¶åˆ—è¡¨
        output_files = get_output_files(crawler_config['platform'], task_id)
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        tasks[task_id]['status'] = TaskStatus.COMPLETED
        tasks[task_id]['message'] = 'çˆ¬å–ä»»åŠ¡å®Œæˆï¼'
        tasks[task_id]['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        tasks[task_id]['completed_at'] = time.time()
        tasks[task_id]['output_files'] = output_files
        
    except Exception as e:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        tasks[task_id]['status'] = TaskStatus.FAILED
        tasks[task_id]['error'] = str(e)
        tasks[task_id]['message'] = 'çˆ¬å–ä»»åŠ¡å¤±è´¥ï¼'
        tasks[task_id]['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        tasks[task_id]['completed_at'] = time.time()
        raise
    
    finally:
        # æ¸…ç†èµ„æº
        if crawler:
            try:
                # å¦‚æœçˆ¬è™«æœ‰æ¸…ç†æ–¹æ³•ï¼Œè°ƒç”¨å®ƒ
                if hasattr(crawler, 'close'):
                    await crawler.close()
            except Exception as e:
                print(f"Error closing crawler: {e}")
        
        # å…³é—­æ•°æ®åº“è¿æ¥
        if config.SAVE_DATA_OPTION in ["db", "sqlite"]:
            try:
                await db.close()
            except Exception as e:
                print(f"Error closing database: {e}")


def get_output_files(platform: str, task_id: str) -> list:
    """è·å–ä»»åŠ¡ç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶åˆ—è¡¨"""
    output_files = []
    data_dir = "data"
    
    if not os.path.exists(data_dir):
        return output_files
    
    # æŸ¥æ‰¾ä¸ä»»åŠ¡ç›¸å…³çš„æ–‡ä»¶
    for filename in os.listdir(data_dir):
        if platform in filename.lower():
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ˜¯åœ¨ä»»åŠ¡å¼€å§‹ååˆ›å»ºçš„
            file_path = os.path.join(data_dir, filename)
            if os.path.isfile(file_path):
                file_mtime = os.path.getmtime(file_path)
                task_start_time = tasks.get(task_id, {}).get('started_at', 0)
                if file_mtime >= task_start_time:
                    output_files.append(filename)
    
    return output_files

@app.route('/')
def index():
    """ä¸»é¡µ"""
    return render_template('index.html', 
                         platforms=PLATFORM_OPTIONS,
                         login_types=LOGIN_TYPE_OPTIONS,
                         crawler_types=CRAWLER_TYPE_OPTIONS,
                         save_data_options=SAVE_DATA_OPTIONS)

@app.route('/start_task', methods=['POST'])
def start_task():
    """å¯åŠ¨çˆ¬è™«ä»»åŠ¡"""
    try:
        # è·å–è¡¨å•æ•°æ®
        crawler_config = {
            'platform': request.form.get('platform', 'xhs'),
            'login_type': request.form.get('login_type', 'qrcode'),
            'crawler_type': request.form.get('crawler_type', 'search'),
            'keywords': request.form.get('keywords', ''),
            'creator_id': request.form.get('creator_id', ''),
            'start_page': int(request.form.get('start_page', 1)),
            'get_comments': request.form.get('get_comments') == 'on',
            'get_sub_comments': request.form.get('get_sub_comments') == 'on',
            'save_data_option': request.form.get('save_data_option', 'json'),
            'max_notes_count': int(request.form.get('max_notes_count', 20)),
            'cookies': request.form.get('cookies', '')
        }
        
        
        # éªŒè¯å¿…è¦å‚æ•°
        if crawler_config['crawler_type'] == 'search' and not crawler_config['keywords']:
            return jsonify({'success': False, 'message': 'æœç´¢æ¨¡å¼ä¸‹å¿…é¡»è¾“å…¥å…³é”®è¯ï¼'})
        
        if crawler_config['crawler_type'] == 'creator' and not crawler_config['creator_id']:
            return jsonify({'success': False, 'message': 'åˆ›ä½œè€…æ¨¡å¼ä¸‹å¿…é¡»è¾“å…¥ç”¨æˆ·IDï¼'})
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # åˆ›å»ºä»»åŠ¡è®°å½•
        tasks[task_id] = {
            'id': task_id,
            'status': TaskStatus.PENDING,
            'config': crawler_config,
            'create_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'start_time': None,
            'end_time': None,
            'message': 'ä»»åŠ¡å·²åˆ›å»ºï¼Œç­‰å¾…æ‰§è¡Œ...',
            'error': None
        }
        
        # åœ¨åå°çº¿ç¨‹ä¸­å¯åŠ¨çˆ¬è™«ä»»åŠ¡
        thread = threading.Thread(target=run_crawler_task, args=(task_id, crawler_config))
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True, 
            'task_id': task_id,
            'message': 'ä»»åŠ¡å·²å¯åŠ¨ï¼Œè¯·æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€é¡µé¢è·å–è¿›åº¦ä¿¡æ¯ã€‚'
        })
        
    except Exception as e:
        return jsonify({'success': False, 'message': f'å¯åŠ¨ä»»åŠ¡å¤±è´¥: {str(e)}'})

@app.route('/task_status/<task_id>')
def get_task_status(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if task_id not in tasks:
        return jsonify({'success': False, 'message': 'ä»»åŠ¡ä¸å­˜åœ¨'})
    
    task = tasks[task_id]
    return jsonify({
        'success': True,
        'task': task
    })

@app.route('/api/tasks/<task_id>')
def get_task_detail(task_id):
    """è·å–ä»»åŠ¡è¯¦æƒ…"""
    task = tasks.get(task_id)
    if not task:
        return jsonify({'success': False, 'message': 'ä»»åŠ¡ä¸å­˜åœ¨'})
    
    # æ·»åŠ è¾“å‡ºæ–‡ä»¶ä¿¡æ¯
    task_copy = task.copy()
    if 'output_files' not in task_copy and task_copy['status'] == TaskStatus.COMPLETED:
        task_copy['output_files'] = get_output_files(task_copy['config']['platform'], task_id)
    
    return jsonify({
        'success': True,
        'task': task_copy
    })

@app.route('/tasks')
def list_tasks():
    """ä»»åŠ¡åˆ—è¡¨é¡µé¢"""
    return render_template('tasks.html', tasks=tasks)

@app.route('/api/tasks')
def api_list_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡çš„APIæ¥å£"""
    return jsonify({'success': True, 'tasks': list(tasks.values())})

@app.route('/stop_task/<task_id>', methods=['POST'])
def stop_task(task_id):
    """åœæ­¢ä»»åŠ¡ï¼ˆæ³¨æ„ï¼šç”±äºçˆ¬è™«çš„å¼‚æ­¥ç‰¹æ€§ï¼Œå¯èƒ½æ— æ³•ç«‹å³åœæ­¢ï¼‰"""
    if task_id not in tasks:
        return jsonify({'success': False, 'message': 'ä»»åŠ¡ä¸å­˜åœ¨'})
    
    task = tasks[task_id]
    if task['status'] == TaskStatus.RUNNING:
        task['status'] = TaskStatus.STOPPED
        task['message'] = 'ä»»åŠ¡å·²è¯·æ±‚åœæ­¢ï¼ˆå¯èƒ½éœ€è¦ç­‰å¾…å½“å‰æ“ä½œå®Œæˆï¼‰'
        task['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        return jsonify({'success': True, 'message': 'åœæ­¢è¯·æ±‚å·²å‘é€'})
    else:
        return jsonify({'success': False, 'message': 'ä»»åŠ¡æœªåœ¨è¿è¡Œä¸­'})

@app.route('/data')
def view_data():
    """æ•°æ®æŸ¥çœ‹é¡µé¢"""
    return render_template('data.html')

@app.route('/creators')
def view_creators():
    """åˆ›ä½œè€…æ•°æ®æŸ¥çœ‹é¡µé¢"""
    return render_template('creators.html')

def safe_convert_to_text(value):
    """å®‰å…¨åœ°å°†æ•°å€¼è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œä¿æŒåŸå§‹æ ¼å¼"""
    import pandas as pd
    if pd.isna(value) or value is None:
        return '0'
    return str(value)

def safe_convert_to_int(value):
    """å®‰å…¨åœ°å°†æ•°å€¼è½¬æ¢ä¸ºæ•´æ•°ï¼Œå¤„ç†åŒ…å«ä¸­æ–‡å•ä½çš„æƒ…å†µ"""
    if pd.isna(value) or value is None:
        return 0
    
    # å¦‚æœå·²ç»æ˜¯æ•°å­—ç±»å‹ï¼Œç›´æ¥è¿”å›
    if isinstance(value, (int, float)):
        return int(value)
    
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¤„ç†
    str_value = str(value).strip()
    
    # å¦‚æœæ˜¯ç©ºå­—ç¬¦ä¸²ï¼Œè¿”å›0
    if not str_value:
        return 0
    
    # å°è¯•ç›´æ¥è½¬æ¢ä¸ºæ•´æ•°
    try:
        return int(float(str_value))
    except ValueError:
        # å¦‚æœåŒ…å«ä¸­æ–‡å•ä½ï¼Œè¿”å›0ï¼ˆæˆ–è€…å¯ä»¥é€‰æ‹©ä¿æŒåŸå§‹å­—ç¬¦ä¸²ï¼‰
        return 0

@app.route('/api/creators')
def get_creators():
    """è·å–åˆ›ä½œè€…åˆ—è¡¨API"""
    try:
        import pandas as pd
        import glob
        import json
        from datetime import datetime
        
        # æŸ¥æ‰¾åˆ›ä½œè€…è¯¦ç»†ä¿¡æ¯æ–‡ä»¶
        creator_files = glob.glob('data/xhs/*_creator_creator_*.csv')
        if not creator_files:
            return jsonify({'success': True, 'creators': []})
        
        # è¯»å–æ‰€æœ‰åˆ›ä½œè€…æ–‡ä»¶å¹¶åˆå¹¶
        all_creator_data = []
        for file_path in creator_files:
            try:
                df = pd.read_csv(file_path)
                # æ·»åŠ æ–‡ä»¶ä¿®æ”¹æ—¶é—´ä½œä¸ºæ•°æ®æ›´æ–°æ—¶é—´
                file_mtime = os.path.getmtime(file_path)
                df['data_update_time'] = file_mtime
                all_creator_data.append(df)
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
                continue
        
        if not all_creator_data:
            return jsonify({'success': True, 'creators': []})
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        creator_df = pd.concat(all_creator_data, ignore_index=True)
        
        # æŒ‰user_idåˆ†ç»„ï¼Œä¿ç•™æœ€æ–°çš„æ•°æ®ï¼ˆåŸºäºdata_update_timeï¼‰
        # ä½†ç¡®ä¿æ‰€æœ‰åˆ›ä½œè€…éƒ½èƒ½æ˜¾ç¤ºï¼Œå³ä½¿ä»–ä»¬ä¸åœ¨æœ€æ–°æ–‡ä»¶ä¸­
        creator_df = creator_df.sort_values('data_update_time').groupby('user_id').tail(1).reset_index(drop=True)
        
        # æŸ¥æ‰¾å†…å®¹æ–‡ä»¶æ¥è·å–ç¬”è®°æ•°é‡
        content_files = glob.glob('data/xhs/*_creator_contents_*.csv')
        note_counts = {}
        if content_files:
            all_content_data = []
            for file_path in content_files:
                try:
                    df = pd.read_csv(file_path)
                    all_content_data.append(df)
                except Exception as e:
                    continue
            
            if all_content_data:
                content_df = pd.concat(all_content_data, ignore_index=True)
                # å»é‡å¹¶ç»Ÿè®¡ç¬”è®°æ•°é‡
                content_df = content_df.drop_duplicates(subset=['note_id'])
                note_counts = content_df.groupby('user_id').size().to_dict()
        
        creators = []
        for _, row in creator_df.iterrows():
            # è§£ætag_list JSONå­—ç¬¦ä¸²
            tag_info = {}
            try:
                if pd.notna(row.get('tag_list')) and row['tag_list'].strip():
                    tag_info = json.loads(row['tag_list'])
            except (json.JSONDecodeError, AttributeError):
                tag_info = {}
            
            # æ ¼å¼åŒ–æ•°æ®æ›´æ–°æ—¶é—´
            update_time = datetime.fromtimestamp(row['data_update_time']).strftime('%Y-%m-%d %H:%M:%S')
            
            # å®‰å…¨å¤„ç†å¯èƒ½åŒ…å«NaNçš„å­—æ®µ
            def safe_get_value(value, default=''):
                if pd.isna(value) or value is None:
                    return default
                return str(value) if value != '' else default
            
            creators.append({
                'user_id': safe_get_value(row['user_id']),
                'nickname': safe_get_value(row['nickname']),
                'avatar': safe_get_value(row['avatar']),
                'desc': safe_get_value(row.get('desc', '')),  # ç®€ä»‹
                'ip_location': safe_get_value(row.get('ip_location', '')),
                'follows': safe_convert_to_int(row.get('follows', 0)),  # å…³æ³¨æ•°
                'fans': safe_convert_to_int(row.get('fans', 0)),  # ç²‰ä¸æ•°
                'interaction': safe_convert_to_int(row.get('interaction', 0)),  # æ€»äº’åŠ¨æ•°
                'note_count': note_counts.get(row['user_id'], 0),  # ç¬”è®°æ•°é‡
                'last_modify_ts': safe_convert_to_int(row.get('last_modify_ts', 0)),
                'data_update_time': update_time,  # æ•°æ®æ›´æ–°æ—¶é—´
                'gender': safe_get_value(row.get('gender', '')),  # æ€§åˆ«
                'tag_list': tag_info,  # æ ‡ç­¾ä¿¡æ¯
                # é¢å¤–ä¿¡æ¯ï¼ˆå¹´é¾„ã€åœ°åŒºã€èŒä¸šã€å­¦æ ¡ç­‰ï¼‰
                'age': tag_info.get('info', ''),
                'location': tag_info.get('location', ''),
                'profession': tag_info.get('profession', ''),
                'college': tag_info.get('college', '')
            })
        
        # æŒ‰ç²‰ä¸æ•°æ’åº
        creators.sort(key=lambda x: x['fans'], reverse=True)
        
        return jsonify({'success': True, 'creators': creators})
    except Exception as e:
        return jsonify({'success': False, 'message': f'åŠ è½½åˆ›ä½œè€…æ•°æ®å¤±è´¥: {str(e)}'})

@app.route('/api/creator/<user_id>/notes')
def get_creator_notes(user_id):
    """è·å–æŒ‡å®šåˆ›ä½œè€…çš„ç¬”è®°åˆ—è¡¨API"""
    try:
        import pandas as pd
        import glob
        
        # è·å–æ’åºå‚æ•°
        sort_by = request.args.get('sort_by', 'time')  # é»˜è®¤æŒ‰æ—¶é—´æ’åº
        sort_order = request.args.get('sort_order', 'desc')  # é»˜è®¤é™åº
        
        # æŸ¥æ‰¾æ‰€æœ‰å°çº¢ä¹¦CSVæ–‡ä»¶
        csv_files = glob.glob('data/xhs/*_creator_contents_*.csv')
        if not csv_files:
            return jsonify({'success': True, 'notes': []})
        
        # è¯»å–æ‰€æœ‰CSVæ–‡ä»¶å¹¶åˆå¹¶
        all_content_data = []
        for file_path in csv_files:
            try:
                df = pd.read_csv(file_path)
                all_content_data.append(df)
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
                continue
        
        if not all_content_data:
            return jsonify({'success': True, 'notes': []})
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        df = pd.concat(all_content_data, ignore_index=True)
        
        # å»é‡ï¼Œä¿ç•™æ¯ä¸ªnote_idçš„æœ€æ–°æ•°æ®
        df = df.drop_duplicates(subset=['note_id'], keep='last')
        
        # ç­›é€‰æŒ‡å®šåˆ›ä½œè€…çš„ç¬”è®°
        user_notes = df[df['user_id'] == user_id].copy()
        
        # å¤„ç†æ’åºå­—æ®µçš„æ•°æ®ç±»å‹è½¬æ¢
        def convert_count_to_number(value):
            """å°†åŒ…å«ä¸­æ–‡å•ä½çš„æ•°å€¼è½¬æ¢ä¸ºæ•°å­—ç”¨äºæ’åº"""
            if pd.isna(value) or value == '':
                return 0
            
            str_value = str(value)
            if 'ä¸‡' in str_value:
                try:
                    return float(str_value.replace('ä¸‡', '')) * 10000
                except:
                    return 0
            elif 'åƒ' in str_value:
                try:
                    return float(str_value.replace('åƒ', '')) * 1000
                except:
                    return 0
            else:
                try:
                    return float(str_value)
                except:
                    return 0
        
        # æ ¹æ®æ’åºå­—æ®µè¿›è¡Œæ•°æ®å¤„ç†
        if sort_by in ['liked_count', 'collected_count', 'comment_count', 'share_count']:
            # ä¸ºæ•°å€¼å­—æ®µåˆ›å»ºä¸´æ—¶æ’åºåˆ—ï¼Œç›´æ¥è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
            user_notes[f'{sort_by}_numeric'] = pd.to_numeric(user_notes[sort_by], errors='coerce').fillna(0)
            sort_column = f'{sort_by}_numeric'
        elif sort_by == 'time':
            # æ—¶é—´å­—æ®µè½¬æ¢ä¸ºæ•°å€¼ç±»å‹è¿›è¡Œæ’åº
            user_notes['time_numeric'] = pd.to_numeric(user_notes['time'], errors='coerce').fillna(0)
            sort_column = 'time_numeric'
        else:
            sort_column = sort_by
        
        # æ‰§è¡Œæ’åº
        ascending = (sort_order == 'asc')
        if sort_column in user_notes.columns:
            user_notes = user_notes.sort_values(sort_column, ascending=ascending)
        else:
            # å¦‚æœæ’åºå­—æ®µä¸å­˜åœ¨ï¼Œé»˜è®¤æŒ‰æ—¶é—´æ’åº
            user_notes['time_numeric'] = pd.to_numeric(user_notes['time'], errors='coerce').fillna(0)
            user_notes = user_notes.sort_values('time_numeric', ascending=False)
        
        notes = []
        for _, row in user_notes.iterrows():
            notes.append({
                'note_id': row['note_id'],
                'title': row['title'] if pd.notna(row['title']) else '',
                'desc': row['desc'] if pd.notna(row['desc']) else '',
                'type': row['type'] if pd.notna(row['type']) else '',
                'video_url': row['video_url'] if pd.notna(row['video_url']) else '',
                'time': str(int(row['time'])) if pd.notna(row['time']) else '',
                'last_update_time': str(int(row['last_update_time'])) if pd.notna(row['last_update_time']) else '',
                'liked_count': safe_convert_to_text(row['liked_count']),
                'collected_count': safe_convert_to_text(row['collected_count']),
                'comment_count': safe_convert_to_text(row['comment_count']),
                'share_count': safe_convert_to_text(row['share_count']),
                'image_list': row['image_list'] if pd.notna(row['image_list']) else '',
                'tag_list': row['tag_list'] if pd.notna(row['tag_list']) else '',
                'note_url': row['note_url'] if pd.notna(row['note_url']) else '',
                'ip_location': row['ip_location'] if pd.notna(row['ip_location']) else ''
            })
        
        return jsonify({'success': True, 'notes': notes})
    except Exception as e:
        return jsonify({'success': False, 'message': f'åŠ è½½ç¬”è®°æ•°æ®å¤±è´¥: {str(e)}'})

@app.route('/api/data_files')
def list_data_files():
    """è·å–æ•°æ®æ–‡ä»¶åˆ—è¡¨"""
    try:
        data_dir = 'data'
        if not os.path.exists(data_dir):
            return jsonify({'success': True, 'files': []})
        
        files = []
        for filename in os.listdir(data_dir):
            if filename.endswith(('.json', '.csv')):
                filepath = os.path.join(data_dir, filename)
                stat = os.stat(filepath)
                files.append({
                    'name': filename,
                    'size': stat.st_size,
                    'modified': datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
                })
        
        return jsonify({'success': True, 'files': files})
    except Exception as e:
        return jsonify({'success': False, 'message': str(e)})

@app.route('/api/data/files')
def get_data_files():
    """è·å–æ•°æ®æ–‡ä»¶åˆ—è¡¨"""
    try:
        data_dir = 'data'
        if not os.path.exists(data_dir):
            os.makedirs(data_dir, exist_ok=True)
            return jsonify({'success': True, 'files': []})
        
        files = []
        for filename in os.listdir(data_dir):
            file_path = os.path.join(data_dir, filename)
            if os.path.isfile(file_path):
                stat = os.stat(file_path)
                files.append({
                    'name': filename,
                    'size': stat.st_size,
                    'modified_time': stat.st_mtime
                })
        
        # æŒ‰ä¿®æ”¹æ—¶é—´å€’åºæ’åˆ—
        files.sort(key=lambda x: x['modified_time'], reverse=True)
        
        return jsonify({'success': True, 'files': files})
    except Exception as e:
        return jsonify({'success': False, 'message': str(e)})

@app.route('/download/<filename>')
def download_file(filename):
    """ä¸‹è½½æ•°æ®æ–‡ä»¶"""
    try:
        # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢è·¯å¾„éå†æ”»å‡»
        if '..' in filename or '/' in filename or '\\' in filename:
            return jsonify({'success': False, 'message': 'éæ³•æ–‡ä»¶å'}), 400
        
        file_path = os.path.join('data', filename)
        if not os.path.exists(file_path):
            return jsonify({'success': False, 'message': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        return send_file(file_path, as_attachment=True, download_name=filename)
    except Exception as e:
        return jsonify({'success': False, 'message': str(e)}), 500

if __name__ == '__main__':
    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    os.makedirs('web_templates', exist_ok=True)
    os.makedirs('web_static', exist_ok=True)
    os.makedirs('data', exist_ok=True)
    
    print("\n" + "="*50)
    print("ğŸš€ MediaCrawler Web ç•Œé¢å·²å¯åŠ¨ï¼")
    print("ğŸ“± è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://localhost:5000")
    print("ğŸ’¡ ä½¿ç”¨Webç•Œé¢å¯ä»¥æ›´æ–¹ä¾¿åœ°é…ç½®å’Œç®¡ç†çˆ¬è™«ä»»åŠ¡")
    print("="*50 + "\n")
    
    app.run(debug=True, host='0.0.0.0', port=5000, threaded=True)