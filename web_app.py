# -*- coding: utf-8 -*-
# å£°æ˜ï¼šæœ¬ä»£ç ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚ä½¿ç”¨è€…åº”éµå®ˆä»¥ä¸‹åŸåˆ™ï¼š
# 1. ä¸å¾—ç”¨äºä»»ä½•å•†ä¸šç”¨é€”ã€‚
# 2. ä½¿ç”¨æ—¶åº”éµå®ˆç›®æ ‡å¹³å°çš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtè§„åˆ™ã€‚
# 3. ä¸å¾—è¿›è¡Œå¤§è§„æ¨¡çˆ¬å–æˆ–å¯¹å¹³å°é€ æˆè¿è¥å¹²æ‰°ã€‚
# 4. åº”åˆç†æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…ç»™ç›®æ ‡å¹³å°å¸¦æ¥ä¸å¿…è¦çš„è´Ÿæ‹…ã€‚
# 5. ä¸å¾—ç”¨äºä»»ä½•éæ³•æˆ–ä¸å½“çš„ç”¨é€”ã€‚
#
# è¯¦ç»†è®¸å¯æ¡æ¬¾è¯·å‚é˜…é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„LICENSEæ–‡ä»¶ã€‚
# ä½¿ç”¨æœ¬ä»£ç å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä¸Šè¿°åŸåˆ™å’ŒLICENSEä¸­çš„æ‰€æœ‰æ¡æ¬¾ã€‚

from flask import Flask, render_template, request, jsonify, send_file
import asyncio
import threading
import uuid
import os
import json
import time
from datetime import datetime
import sys
from typing import Dict, Any, Optional

# å¯¼å…¥åŸæœ‰çš„çˆ¬è™«æ¨¡å—
import config
import db
from main import CrawlerFactory
from base.base_crawler import AbstractCrawler

app = Flask(__name__, template_folder='web_templates', static_folder='web_static')

# å­˜å‚¨ä»»åŠ¡çŠ¶æ€çš„å­—å…¸
tasks = {}

# å¹³å°é…ç½®
PLATFORM_OPTIONS = {
    'xhs': 'å°çº¢ä¹¦',
    'dy': 'æŠ–éŸ³',
    'ks': 'å¿«æ‰‹',
    'bili': 'å“”å“©å“”å“©',
    'wb': 'å¾®åš',
    'tieba': 'ç™¾åº¦è´´å§',
    'zhihu': 'çŸ¥ä¹'
}

# ç™»å½•æ–¹å¼é…ç½®
LOGIN_TYPE_OPTIONS = {
    'qrcode': 'äºŒç»´ç ç™»å½•',
    'phone': 'æ‰‹æœºå·ç™»å½•',
    'cookie': 'Cookieç™»å½•'
}

# çˆ¬å–ç±»å‹é…ç½®
CRAWLER_TYPE_OPTIONS = {
    'search': 'å…³é”®è¯æœç´¢',
    'detail': 'å¸–å­è¯¦æƒ…',
    'creator': 'åˆ›ä½œè€…ä¸»é¡µ'
}

# æ•°æ®ä¿å­˜æ–¹å¼é…ç½®
SAVE_DATA_OPTIONS = {
    'json': 'JSONæ–‡ä»¶',
    'csv': 'CSVæ–‡ä»¶',
    'sqlite': 'SQLiteæ•°æ®åº“',
    'db': 'MySQLæ•°æ®åº“'
}

class TaskStatus:
    """ä»»åŠ¡çŠ¶æ€ç±»"""
    PENDING = 'pending'      # ç­‰å¾…ä¸­
    RUNNING = 'running'      # è¿è¡Œä¸­
    COMPLETED = 'completed'  # å·²å®Œæˆ
    FAILED = 'failed'        # å¤±è´¥
    STOPPED = 'stopped'      # å·²åœæ­¢

def run_crawler_task(task_id: str, crawler_config: Dict[str, Any]):
    """åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œçˆ¬è™«ä»»åŠ¡"""
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
        tasks[task_id]['status'] = TaskStatus.RUNNING
        tasks[task_id]['start_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        tasks[task_id]['started_at'] = time.time()
        
        # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯æ¥è¿è¡Œå¼‚æ­¥çˆ¬è™«ä»»åŠ¡
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(execute_crawler_task(task_id, crawler_config))
        finally:
            loop.close()
        
    except Exception as e:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        tasks[task_id]['status'] = TaskStatus.FAILED
        tasks[task_id]['error'] = str(e)
        tasks[task_id]['message'] = f'ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {str(e)}'
        tasks[task_id]['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        tasks[task_id]['completed_at'] = time.time()
        print(f"Task {task_id} failed with error: {e}")


async def execute_crawler_task(task_id: str, crawler_config: Dict[str, Any]):
    """æ‰§è¡Œçˆ¬è™«ä»»åŠ¡çš„å¼‚æ­¥å‡½æ•°"""
    crawler: Optional[AbstractCrawler] = None
    
    try:
        # è®¾ç½®é…ç½®å‚æ•°
        config.PLATFORM = crawler_config['platform']
        config.LOGIN_TYPE = crawler_config['login_type']
        config.CRAWLER_TYPE = crawler_config['crawler_type']
        config.KEYWORDS = crawler_config['keywords']
        config.START_PAGE = crawler_config['start_page']
        config.ENABLE_GET_COMMENTS = crawler_config['get_comments']
        config.ENABLE_GET_SUB_COMMENTS = crawler_config['get_sub_comments']
        config.SAVE_DATA_OPTION = crawler_config['save_data_option']
        config.CRAWLER_MAX_NOTES_COUNT = crawler_config['max_notes_count']
        config.COOKIES = crawler_config.get('cookies', '')
        
        # å¤„ç†åˆ›ä½œè€…æ¨¡å¼çš„ç”¨æˆ·ID
        if crawler_config['crawler_type'] == 'creator' and crawler_config.get('creator_id'):
            # å°†ç”¨æˆ·è¾“å…¥çš„åˆ›ä½œè€…IDè®¾ç½®åˆ°é…ç½®ä¸­
            if crawler_config['platform'] == 'xhs':
                config.XHS_CREATOR_ID_LIST = [crawler_config['creator_id']]
        
        # åˆå§‹åŒ–æ•°æ®åº“ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if config.SAVE_DATA_OPTION in ["db", "sqlite"]:
            await db.init_db()
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = CrawlerFactory.create_crawler(platform=config.PLATFORM)
        
        # å¯åŠ¨çˆ¬è™«
        await crawler.start()
        
        # è·å–è¾“å‡ºæ–‡ä»¶åˆ—è¡¨
        output_files = get_output_files(crawler_config['platform'], task_id)
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        tasks[task_id]['status'] = TaskStatus.COMPLETED
        tasks[task_id]['message'] = 'çˆ¬å–ä»»åŠ¡å®Œæˆï¼'
        tasks[task_id]['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        tasks[task_id]['completed_at'] = time.time()
        tasks[task_id]['output_files'] = output_files
        
    except Exception as e:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        tasks[task_id]['status'] = TaskStatus.FAILED
        tasks[task_id]['error'] = str(e)
        tasks[task_id]['message'] = 'çˆ¬å–ä»»åŠ¡å¤±è´¥ï¼'
        tasks[task_id]['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        tasks[task_id]['completed_at'] = time.time()
        raise
    
    finally:
        # æ¸…ç†èµ„æº
        if crawler:
            try:
                # å¦‚æœçˆ¬è™«æœ‰æ¸…ç†æ–¹æ³•ï¼Œè°ƒç”¨å®ƒ
                if hasattr(crawler, 'close'):
                    await crawler.close()
            except Exception as e:
                print(f"Error closing crawler: {e}")
        
        # å…³é—­æ•°æ®åº“è¿æ¥
        if config.SAVE_DATA_OPTION in ["db", "sqlite"]:
            try:
                await db.close()
            except Exception as e:
                print(f"Error closing database: {e}")


def get_output_files(platform: str, task_id: str) -> list:
    """è·å–ä»»åŠ¡ç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶åˆ—è¡¨"""
    output_files = []
    data_dir = "data"
    
    if not os.path.exists(data_dir):
        return output_files
    
    # æŸ¥æ‰¾ä¸ä»»åŠ¡ç›¸å…³çš„æ–‡ä»¶
    for filename in os.listdir(data_dir):
        if platform in filename.lower():
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ˜¯åœ¨ä»»åŠ¡å¼€å§‹ååˆ›å»ºçš„
            file_path = os.path.join(data_dir, filename)
            if os.path.isfile(file_path):
                file_mtime = os.path.getmtime(file_path)
                task_start_time = tasks.get(task_id, {}).get('started_at', 0)
                if file_mtime >= task_start_time:
                    output_files.append(filename)
    
    return output_files

@app.route('/')
def index():
    """ä¸»é¡µ"""
    return render_template('index.html', 
                         platforms=PLATFORM_OPTIONS,
                         login_types=LOGIN_TYPE_OPTIONS,
                         crawler_types=CRAWLER_TYPE_OPTIONS,
                         save_data_options=SAVE_DATA_OPTIONS)

@app.route('/start_task', methods=['POST'])
def start_task():
    """å¯åŠ¨çˆ¬è™«ä»»åŠ¡"""
    try:
        # è·å–è¡¨å•æ•°æ®
        crawler_config = {
            'platform': request.form.get('platform', 'xhs'),
            'login_type': request.form.get('login_type', 'qrcode'),
            'crawler_type': request.form.get('crawler_type', 'search'),
            'keywords': request.form.get('keywords', ''),
            'creator_id': request.form.get('creator_id', ''),
            'start_page': int(request.form.get('start_page', 1)),
            'get_comments': request.form.get('get_comments') == 'on',
            'get_sub_comments': request.form.get('get_sub_comments') == 'on',
            'save_data_option': request.form.get('save_data_option', 'json'),
            'max_notes_count': int(request.form.get('max_notes_count', 20)),
            'cookies': request.form.get('cookies', '')
        }
        
        
        # éªŒè¯å¿…è¦å‚æ•°
        if crawler_config['crawler_type'] == 'search' and not crawler_config['keywords']:
            return jsonify({'success': False, 'message': 'æœç´¢æ¨¡å¼ä¸‹å¿…é¡»è¾“å…¥å…³é”®è¯ï¼'})
        
        if crawler_config['crawler_type'] == 'creator' and not crawler_config['creator_id']:
            return jsonify({'success': False, 'message': 'åˆ›ä½œè€…æ¨¡å¼ä¸‹å¿…é¡»è¾“å…¥ç”¨æˆ·IDï¼'})
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # åˆ›å»ºä»»åŠ¡è®°å½•
        tasks[task_id] = {
            'id': task_id,
            'status': TaskStatus.PENDING,
            'config': crawler_config,
            'create_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'start_time': None,
            'end_time': None,
            'message': 'ä»»åŠ¡å·²åˆ›å»ºï¼Œç­‰å¾…æ‰§è¡Œ...',
            'error': None
        }
        
        # åœ¨åå°çº¿ç¨‹ä¸­å¯åŠ¨çˆ¬è™«ä»»åŠ¡
        thread = threading.Thread(target=run_crawler_task, args=(task_id, crawler_config))
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True, 
            'task_id': task_id,
            'message': 'ä»»åŠ¡å·²å¯åŠ¨ï¼Œè¯·æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€é¡µé¢è·å–è¿›åº¦ä¿¡æ¯ã€‚'
        })
        
    except Exception as e:
        return jsonify({'success': False, 'message': f'å¯åŠ¨ä»»åŠ¡å¤±è´¥: {str(e)}'})

@app.route('/task_status/<task_id>')
def get_task_status(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if task_id not in tasks:
        return jsonify({'success': False, 'message': 'ä»»åŠ¡ä¸å­˜åœ¨'})
    
    task = tasks[task_id]
    return jsonify({
        'success': True,
        'task': task
    })

@app.route('/api/tasks/<task_id>')
def get_task_detail(task_id):
    """è·å–ä»»åŠ¡è¯¦æƒ…"""
    task = tasks.get(task_id)
    if not task:
        return jsonify({'success': False, 'message': 'ä»»åŠ¡ä¸å­˜åœ¨'})
    
    # æ·»åŠ è¾“å‡ºæ–‡ä»¶ä¿¡æ¯
    task_copy = task.copy()
    if 'output_files' not in task_copy and task_copy['status'] == TaskStatus.COMPLETED:
        task_copy['output_files'] = get_output_files(task_copy['config']['platform'], task_id)
    
    return jsonify({
        'success': True,
        'task': task_copy
    })

@app.route('/tasks')
def list_tasks():
    """ä»»åŠ¡åˆ—è¡¨é¡µé¢"""
    return render_template('tasks.html', tasks=tasks)

@app.route('/api/tasks')
def api_list_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡çš„APIæ¥å£"""
    return jsonify({'success': True, 'tasks': list(tasks.values())})

@app.route('/stop_task/<task_id>', methods=['POST'])
def stop_task(task_id):
    """åœæ­¢ä»»åŠ¡ï¼ˆæ³¨æ„ï¼šç”±äºçˆ¬è™«çš„å¼‚æ­¥ç‰¹æ€§ï¼Œå¯èƒ½æ— æ³•ç«‹å³åœæ­¢ï¼‰"""
    if task_id not in tasks:
        return jsonify({'success': False, 'message': 'ä»»åŠ¡ä¸å­˜åœ¨'})
    
    task = tasks[task_id]
    if task['status'] == TaskStatus.RUNNING:
        task['status'] = TaskStatus.STOPPED
        task['message'] = 'ä»»åŠ¡å·²è¯·æ±‚åœæ­¢ï¼ˆå¯èƒ½éœ€è¦ç­‰å¾…å½“å‰æ“ä½œå®Œæˆï¼‰'
        task['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        return jsonify({'success': True, 'message': 'åœæ­¢è¯·æ±‚å·²å‘é€'})
    else:
        return jsonify({'success': False, 'message': 'ä»»åŠ¡æœªåœ¨è¿è¡Œä¸­'})

@app.route('/data')
def view_data():
    """æ•°æ®æŸ¥çœ‹é¡µé¢"""
    return render_template('data.html')

@app.route('/api/data_files')
def list_data_files():
    """è·å–æ•°æ®æ–‡ä»¶åˆ—è¡¨"""
    try:
        data_dir = 'data'
        if not os.path.exists(data_dir):
            return jsonify({'success': True, 'files': []})
        
        files = []
        for filename in os.listdir(data_dir):
            if filename.endswith(('.json', '.csv')):
                filepath = os.path.join(data_dir, filename)
                stat = os.stat(filepath)
                files.append({
                    'name': filename,
                    'size': stat.st_size,
                    'modified': datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
                })
        
        return jsonify({'success': True, 'files': files})
    except Exception as e:
        return jsonify({'success': False, 'message': str(e)})

@app.route('/api/data/files')
def get_data_files():
    """è·å–æ•°æ®æ–‡ä»¶åˆ—è¡¨"""
    try:
        data_dir = 'data'
        if not os.path.exists(data_dir):
            os.makedirs(data_dir, exist_ok=True)
            return jsonify({'success': True, 'files': []})
        
        files = []
        for filename in os.listdir(data_dir):
            file_path = os.path.join(data_dir, filename)
            if os.path.isfile(file_path):
                stat = os.stat(file_path)
                files.append({
                    'name': filename,
                    'size': stat.st_size,
                    'modified_time': stat.st_mtime
                })
        
        # æŒ‰ä¿®æ”¹æ—¶é—´å€’åºæ’åˆ—
        files.sort(key=lambda x: x['modified_time'], reverse=True)
        
        return jsonify({'success': True, 'files': files})
    except Exception as e:
        return jsonify({'success': False, 'message': str(e)})

@app.route('/download/<filename>')
def download_file(filename):
    """ä¸‹è½½æ•°æ®æ–‡ä»¶"""
    try:
        # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢è·¯å¾„éå†æ”»å‡»
        if '..' in filename or '/' in filename or '\\' in filename:
            return jsonify({'success': False, 'message': 'éæ³•æ–‡ä»¶å'}), 400
        
        file_path = os.path.join('data', filename)
        if not os.path.exists(file_path):
            return jsonify({'success': False, 'message': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        return send_file(file_path, as_attachment=True, download_name=filename)
    except Exception as e:
        return jsonify({'success': False, 'message': str(e)}), 500

if __name__ == '__main__':
    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    os.makedirs('web_templates', exist_ok=True)
    os.makedirs('web_static', exist_ok=True)
    os.makedirs('data', exist_ok=True)
    
    print("\n" + "="*50)
    print("ğŸš€ MediaCrawler Web ç•Œé¢å·²å¯åŠ¨ï¼")
    print("ğŸ“± è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://localhost:5000")
    print("ğŸ’¡ ä½¿ç”¨Webç•Œé¢å¯ä»¥æ›´æ–¹ä¾¿åœ°é…ç½®å’Œç®¡ç†çˆ¬è™«ä»»åŠ¡")
    print("="*50 + "\n")
    
    app.run(debug=True, host='0.0.0.0', port=5000, threaded=True)