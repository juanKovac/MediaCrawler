# MediaCrawler 本地部署使用说明

## 📋 项目简介

MediaCrawler 是一个功能强大的多平台自媒体数据采集工具，支持小红书、抖音、快手、B站、微博、贴吧、知乎等主流平台的公开信息抓取。

## ✅ 部署状态

✅ **项目已成功部署完成！**

- ✅ 项目代码已克隆到本地
- ✅ uv 包管理工具已安装 (v0.8.9)
- ✅ Node.js 环境已就绪 (v22.15.0)
- ✅ Python 依赖包已安装完成
- ✅ Playwright 浏览器驱动已安装

## 🚀 快速开始

### 1. 基本使用命令

```bash
# 小红书关键词搜索（推荐新手使用）
uv run main.py --platform xhs --lt qrcode --type search --keywords "编程学习" --save_data_option json

# 抖音关键词搜索
uv run main.py --platform dy --lt qrcode --type search --keywords "编程教程" --save_data_option json

# B站关键词搜索
uv run main.py --platform bili --lt qrcode --type search --keywords "Python教程" --save_data_option json
```

### 2. 支持的平台

| 平台代码 | 平台名称 | 功能支持 |
|---------|----------|----------|
| xhs     | 小红书   | ✅ 关键词搜索、指定帖子、创作者主页、评论爬取 |
| dy      | 抖音     | ✅ 关键词搜索、指定帖子、创作者主页、评论爬取 |
| ks      | 快手     | ✅ 关键词搜索、指定帖子、创作者主页、评论爬取 |
| bili    | B站      | ✅ 关键词搜索、指定帖子、创作者主页、评论爬取 |
| wb      | 微博     | ✅ 关键词搜索、指定帖子、创作者主页、评论爬取 |
| tieba   | 百度贴吧 | ✅ 关键词搜索、指定帖子、创作者主页、评论爬取 |
| zhihu   | 知乎     | ✅ 关键词搜索、指定帖子、创作者主页、评论爬取 |

### 3. 登录方式

- `qrcode`: 二维码登录（推荐，最安全）
- `phone`: 手机号登录
- `cookie`: Cookie登录（需要手动获取）

### 4. 爬取类型

- `search`: 关键词搜索（根据关键词搜索相关内容）
- `detail`: 指定帖子详情（爬取特定帖子的详细信息）
- `creator`: 创作者主页（爬取特定用户的所有内容）

### 5. 数据保存方式

- `json`: JSON文件（推荐新手使用，简单易读）
- `csv`: CSV文件（适合Excel分析）
- `sqlite`: SQLite数据库（轻量级数据库）
- `db`: MySQL数据库（需要额外配置）

## ⚙️ 配置说明

### 主要配置文件：`config/base_config.py`

```python
# 基础配置
PLATFORM = "xhs"  # 默认平台
KEYWORDS = "编程副业,编程兼职"  # 搜索关键词
LOGIN_TYPE = "qrcode"  # 登录方式
CRAWLER_TYPE = "search"  # 爬取类型

# 爬取控制
CRAWLER_MAX_NOTES_COUNT = 200  # 最大爬取帖子数量
ENABLE_GET_COMMENTS = True  # 是否爬取评论
CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES = 10  # 单个帖子最大评论数

# 浏览器设置
HEADLESS = False  # 是否无头模式（建议设为False方便调试）
SAVE_LOGIN_STATE = True  # 是否保存登录状态
```

## 📝 使用示例

### 示例1：爬取小红书编程相关内容

```bash
# 1. 修改配置（可选）
# 编辑 config/base_config.py，设置关键词：KEYWORDS = "Python编程,前端开发"

# 2. 运行爬虫
uv run main.py --platform xhs --lt qrcode --type search --save_data_option json

# 3. 扫码登录
# 程序会自动打开浏览器，使用小红书APP扫码登录

# 4. 查看结果
# 数据保存在 data/ 目录下的JSON文件中
```

### 示例2：爬取指定抖音视频的评论

```bash
# 1. 配置视频ID
# 编辑 config/dy_config.py，添加视频ID到 DY_SPECIFIED_ID_LIST

# 2. 运行爬虫
uv run main.py --platform dy --lt qrcode --type detail --get_comment true
```

### 示例3：批量爬取B站UP主视频

```bash
# 1. 配置UP主ID
# 编辑相应配置文件，添加UP主ID

# 2. 运行爬虫
uv run main.py --platform bili --lt qrcode --type creator
```

## ⚠️ 重要注意事项

### 1. 法律合规
- **仅供学习研究使用**，禁止商业用途
- 遵守各平台的使用条款和robots.txt规则
- 不得进行大规模爬取或对平台造成运营干扰
- 合理控制请求频率，避免给平台带来负担

### 2. 使用建议
- 首次使用建议先用小红书测试，相对稳定
- 建议使用二维码登录方式，更安全可靠
- 爬取数量不要设置过大，避免被平台限制
- 如遇到验证码，手动处理后继续

### 3. 常见问题
- **登录失败**：尝试关闭无头模式（HEADLESS = False），手动处理验证
- **数据为空**：检查关键词设置和网络连接
- **程序卡住**：可能遇到反爬机制，稍后重试
- **浏览器报错**：重新安装浏览器驱动：`uv run playwright install`

## 📊 数据输出

爬取的数据会保存在 `data/` 目录下，包含：

- **帖子信息**：标题、内容、作者、发布时间、点赞数等
- **评论信息**：评论内容、评论者、评论时间、点赞数等
- **媒体文件**：图片、视频（需开启媒体爬取功能）

## 🔧 高级功能

### 1. IP代理池
```python
# 在 config/base_config.py 中配置
ENABLE_IP_PROXY = True
IP_PROXY_POOL_COUNT = 2
IP_PROXY_PROVIDER_NAME = "kuaidaili"  # 需要配置代理服务商
```

### 2. 词云生成
```python
# 开启词云功能
ENABLE_GET_WORDCLOUD = True
```

### 3. 媒体文件下载
```python
# 开启媒体下载
ENABLE_GET_MEIDAS = True
```

## 📞 技术支持

- **项目文档**：[https://nanmicoder.github.io/MediaCrawler/](https://nanmicoder.github.io/MediaCrawler/)
- **GitHub仓库**：[https://github.com/NanmiCoder/MediaCrawler](https://github.com/NanmiCoder/MediaCrawler)
- **问题反馈**：GitHub Issues

---

**祝您使用愉快！记住要合法合规使用哦！** 🎉